"Bahrain" = c(21,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA),
"Belgium" = c(29,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA),
"Ethiopia" = c(21,25,17,24,21,22,21,22,21,NA,NA,NA,NA),
"Kenya" = c(21,19,18,22,20,30,20,28,22,20,27,21,21),
"Morocco" = c(32,28,29,24,NA,NA,NA,NA,NA,NA,NA,NA,NA),
"Uganda" = c(21,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA))
View(C6.3_df)
CountryC6.3 <- c("Algeria", "Bahrain", "Belgium", "Ethiopia", "Ethiopia", "Ethiopia", "Ethiopia", "Ethiopia", "Ethiopia",
"Ethiopia", "Ethiopia", "Ethiopia", "Kenya", "Kenya", "Kenya", "Kenya", "Kenya", "Kenya", "Kenya", "Kenya",
"Kenya", "Kenya", "Kenya", "Kenya", "Kenya", "Morocco", "Morocco", "Morocco", "Morocco", "Uganda")
CountryC6.3 <- c("Algeria", "Bahrain", "Belgium", "Ethiopia", "Ethiopia", "Ethiopia", "Ethiopia", "Ethiopia", "Ethiopia",
"Ethiopia", "Ethiopia", "Ethiopia", "Kenya", "Kenya", "Kenya", "Kenya", "Kenya", "Kenya", "Kenya", "Kenya",
"Kenya", "Kenya", "Kenya", "Kenya", "Kenya", "Morocco", "Morocco", "Morocco", "Morocco", "Uganda")
AgeC6.3 <- c(22, 21, 29, 21, 25, 17, 24, 21, 22, 21, 22, 21, 21, 19, 18, 22, 20, 30, 20, 28, 22, 20, 27, 21, 21, 32, 28,
29, 24, 21)
as.data.frame(CountryC6.3, AgeC6.3)
as.data.frame(CountryC6.3, AgeC6.3, row.names=NULL)
CountryC6.3 <- c("Algeria", "Bahrain", "Belgium", "Ethiopia", "Ethiopia", "Ethiopia", "Ethiopia", "Ethiopia", "Ethiopia",
"Ethiopia", "Ethiopia", "Ethiopia", "Kenya", "Kenya", "Kenya", "Kenya", "Kenya", "Kenya", "Kenya", "Kenya",
"Kenya", "Kenya", "Kenya", "Kenya", "Kenya", "Morocco", "Morocco", "Morocco", "Morocco", "Uganda")
AgeC6.3 <- c(22, 21, 29, 21, 25, 17, 24, 21, 22, 21, 22, 21, 21, 19, 18, 22, 20, 30, 20, 28, 22, 20, 27, 21, 21, 32, 28,
29, 24, 21)
C6.3_df <- as.data.frame(CountryC6.3, AgeC6.3, row.names=NULL)
aov.1 = aov(AgeC6.3 ~ as.factor(CountryC6.3), data=C6.3_df)
View(C6.3_df)
col1 <- c("Algeria", "Bahrain", "Belgium", "Ethiopia", "Ethiopia", "Ethiopia", "Ethiopia", "Ethiopia", "Ethiopia",
"Ethiopia", "Ethiopia", "Ethiopia", "Kenya", "Kenya", "Kenya", "Kenya", "Kenya", "Kenya", "Kenya", "Kenya",
"Kenya", "Kenya", "Kenya", "Kenya", "Kenya", "Morocco", "Morocco", "Morocco", "Morocco", "Uganda")
col2 <- c(22, 21, 29, 21, 25, 17, 24, 21, 22, 21, 22, 21, 21, 19, 18, 22, 20, 30, 20, 28, 22, 20, 27, 21, 21, 32, 28,
29, 24, 21)
col1_name <- "country"
col2_name <- "age"
require(reshape2)
library(reshape2)
require(reshape2)
df <- melt(data.frame(col1, col2))
colnames(df) <- c(col1_name, col2_name)
print(df)
col1 <- c("Algeria", "Bahrain", "Belgium", "Ethiopia", "Ethiopia", "Ethiopia", "Ethiopia", "Ethiopia", "Ethiopia",
"Ethiopia", "Ethiopia", "Ethiopia", "Kenya", "Kenya", "Kenya", "Kenya", "Kenya", "Kenya", "Kenya", "Kenya",
"Kenya", "Kenya", "Kenya", "Kenya", "Kenya", "Morocco", "Morocco", "Morocco", "Morocco", "Uganda")
col2 <- c(22, 21, 29, 21, 25, 17, 24, 21, 22, 21, 22, 21, 21, 19, 18, 22, 20, 30, 20, 28, 22, 20, 27, 21, 21, 32, 28,
29, 24, 21)
col1_name <- "country"
col2_name <- "age"
df <- melt(data.frame(col1, col2))
colnames(df) <- c(col1_name, col2_name)
print(df)
col1 <- c("Algeria", "Bahrain", "Belgium", "Ethiopia", "Ethiopia", "Ethiopia", "Ethiopia", "Ethiopia", "Ethiopia",
"Ethiopia", "Ethiopia", "Ethiopia", "Kenya", "Kenya", "Kenya", "Kenya", "Kenya", "Kenya", "Kenya", "Kenya",
"Kenya", "Kenya", "Kenya", "Kenya", "Kenya", "Morocco", "Morocco", "Morocco", "Morocco", "Uganda")
col2 <- c(22, 21, 29, 21, 25, 17, 24, 21, 22, 21, 22, 21, 21, 19, 18, 22, 20, 30, 20, 28, 22, 20, 27, 21, 21, 32, 28,
29, 24, 21)
col1_name <- "country"
col2_name <- "age"
df <- melt(data.frame(col1, col2))
View(df)
col1 <- c("Algeria", "Bahrain", "Belgium", "Ethiopia", "Ethiopia", "Ethiopia", "Ethiopia", "Ethiopia", "Ethiopia",
"Ethiopia", "Ethiopia", "Ethiopia", "Kenya", "Kenya", "Kenya", "Kenya", "Kenya", "Kenya", "Kenya", "Kenya",
"Kenya", "Kenya", "Kenya", "Kenya", "Kenya", "Morocco", "Morocco", "Morocco", "Morocco", "Uganda")
col2 <- c(22, 21, 29, 21, 25, 17, 24, 21, 22, 21, 22, 21, 21, 19, 18, 22, 20, 30, 20, 28, 22, 20, 27, 21, 21, 32, 28,
29, 24, 21)
col1_name <- "country"
col2_name <- "age"
df <- data.frame(col1, col2)
View(df)
colnames(df) <- c(col1_name, col2_name)
print(df)
col1 <- c("Algeria", "Bahrain", "Belgium", "Ethiopia", "Ethiopia", "Ethiopia", "Ethiopia", "Ethiopia", "Ethiopia",
"Ethiopia", "Ethiopia", "Ethiopia", "Kenya", "Kenya", "Kenya", "Kenya", "Kenya", "Kenya", "Kenya", "Kenya",
"Kenya", "Kenya", "Kenya", "Kenya", "Kenya", "Morocco", "Morocco", "Morocco", "Morocco", "Uganda")
col2 <- c(22, 21, 29, 21, 25, 17, 24, 21, 22, 21, 22, 21, 21, 19, 18, 22, 20, 30, 20, 28, 22, 20, 27, 21, 21, 32, 28,
29, 24, 21)
df <- data.frame(col1, col2)
colnames(df) <- c("country", "age")
aov.1 = aov(AgeC6.3 ~ as.factor(CountryC6.3), data=C6.3_df)
aov.1 = aov(age ~ as.factor(country), data=df)
summary(aov.1)
boxplot(age ~ country, data=df)
col1 <- c("Algeria", "Bahrain", "Belgium", "Ethiopia", "Ethiopia", "Ethiopia", "Ethiopia", "Ethiopia", "Ethiopia",
"Ethiopia", "Ethiopia", "Ethiopia", "Kenya", "Kenya", "Kenya", "Kenya", "Kenya", "Kenya", "Kenya", "Kenya",
"Kenya", "Kenya", "Kenya", "Kenya", "Kenya", "Morocco", "Morocco", "Morocco", "Morocco", "Uganda")
col2 <- c(22, 21, 29, 21, 25, 17, 24, 21, 22, 21, 22, 21, 21, 19, 18, 22, 20, 30, 20, 28, 22, 20, 27, 21, 21, 32, 28,
29, 24, 21)
df <- data.frame(col1, col2)
colnames(df) <- c("country", "age")
aov.c6.3 = aov(age ~ as.factor(country), data=df)
summary(aov.c.6.3)
col1 <- c("Algeria", "Bahrain", "Belgium", "Ethiopia", "Ethiopia", "Ethiopia", "Ethiopia", "Ethiopia", "Ethiopia",
"Ethiopia", "Ethiopia", "Ethiopia", "Kenya", "Kenya", "Kenya", "Kenya", "Kenya", "Kenya", "Kenya", "Kenya",
"Kenya", "Kenya", "Kenya", "Kenya", "Kenya", "Morocco", "Morocco", "Morocco", "Morocco", "Uganda")
col2 <- c(22, 21, 29, 21, 25, 17, 24, 21, 22, 21, 22, 21, 21, 19, 18, 22, 20, 30, 20, 28, 22, 20, 27, 21, 21, 32, 28,
29, 24, 21)
df <- data.frame(col1, col2)
colnames(df) <- c("country", "age")
aov.c6.3 <- aov(age ~ as.factor(country), data=df)
summary(aov.c6.3)
sigma <- 15    # theoretical standard deviation
mu0   <- 100   # expected value under H0
mu1   <- 101   # expected value under H1
alpha <- 0.01  # probability of type I error
# critical value for a level alpha test
crit <- qnorm(1-alpha, mu0, sigma)
# power: probability for values > critical value under H1
(pow <- pnorm(crit, mu1, sigma, lower.tail=FALSE))
# probability for type II error: 1 - power
(beta <- 1-pow)
sigma <- 15    # theoretical standard deviation
mu0   <- 100   # expected value under H0
mu1   <- 101   # expected value under H1
alpha <- 0.01  # probability of type I error
# critical value for a level alpha test
crit <- qnorm(1-alpha, mu0, sigma)
# power: probability for values > critical value under H1
(pow <- pnorm(crit, mu1, sigma, lower.tail=TRUE))
# probability for type II error: 1 - power
(beta <- 1-pow)
sigma <- 15    # theoretical standard deviation
mu0   <- 100   # expected value under H0
mu1   <- 100   # expected value under H1
alpha <- 0.01  # probability of type I error
# critical value for a level alpha test
crit <- qnorm(1-alpha, mu0, sigma)
# power: probability for values > critical value under H1
(pow <- pnorm(crit, mu1, sigma, lower.tail=FALSE))
# probability for type II error: 1 - power
(beta <- 1-pow)
sigma <- 15    # theoretical standard deviation
mu0   <- 100   # expected value under H0
mu1   <- 101   # expected value under H1
alpha <- 0.01  # probability of type I error
# critical value for a level alpha test
crit <- qnorm(1-alpha, mu0, sigma)
# power: probability for values > critical value under H1
(pow <- pnorm(crit, mu1, sigma, lower.tail=FALSE))
# probability for type II error: 1 - power
(beta <- 1-pow)
# EXAMPLE 2
n = 100                # sample size
sigma = 15           # population standard deviation
sem = sigma/sqrt(n); sem   # standard error
alpha = .01           # significance level
mu0 = 101          # hypothetical lower bound
q = qnorm(alpha, mean=mu0, sd=sem); q
mu = 100             # assumed actual mean
pnorm(q, mean=mu, sd=sem, lower.tail=FALSE)
n = 100                    # sample size
sigma = 15                 # population standard deviation
sem = sigma/sqrt(n); sem   # standard error
alpha = .01                # significance level
mu0 = 101                  # hypothetical lower bound
q = qnorm(alpha, mean=mu0, sd=sem); q
mu = 100                   # assumed actual mean
pnorm(q, mean=mu, sd=sem, lower.tail=FALSE)
n = 400                    # sample size
sigma = 15                 # population standard deviation
sem = sigma/sqrt(n); sem   # standard error
alpha = .01                # significance level
mu0 = 101                  # hypothetical lower bound
q = qnorm(alpha, mean=mu0, sd=sem); q
mu = 100                   # assumed actual mean
pnorm(q, mean=mu, sd=sem, lower.tail=FALSE)
n = 1600                    # sample size
sigma = 15                 # population standard deviation
sem = sigma/sqrt(n); sem   # standard error
alpha = .01                # significance level
mu0 = 101                  # hypothetical lower bound
q = qnorm(alpha, mean=mu0, sd=sem); q
mu = 100                   # assumed actual mean
pnorm(q, mean=mu, sd=sem, lower.tail=FALSE)
n = 2500                    # sample size
sigma = 15                 # population standard deviation
sem = sigma/sqrt(n); sem   # standard error
alpha = .01                # significance level
mu0 = 101                  # hypothetical lower bound
q = qnorm(alpha, mean=mu0, sd=sem); q
mu = 100                   # assumed actual mean
pnorm(q, mean=mu, sd=sem, lower.tail=FALSE)
# inflated and the remainder (1181-882 = 299) were contricted.
# Is there evidence to contradict Mendel's theory?
# This is a test of 2 proportions (proportion inflated, proportion constricted)
# from one population.
# The null hypothesis is that the proportion inflated is 0.75 and
# the proportion constricted is 0.25.
# The alternative is that the null is not true.
# In lecture we tested this with a "Z-test" and also a chi-square test.
# (With only 2 categories, the z-test and chi-square test are the same.)
# Here, we use the chi-square test.
obscounts = c(882,299)      # Note the data are entered as *counts*.
pi0 = c(0.75, 0.25) # But the null values are given as proportions.
chisq.test(obscounts,p=pi0 ) # This can be confusing. Be careful!
# Text Example 8.9 presents the counts of reasons for which cans do not meet
# specification (blemish on can, crack in can, improper pull tab, pull tab missing,
# other) from 3 different can production lines (lines 1, 2, 3)
# The observed counts are provided as obscounts below.
# The rows correspond to production lines 1, 2 and 3.
# The columns correspond to the 5 reasons: blemish on can, crack in can,
# improper pull tab, pull tab missing, other
obscounts<-matrix(c(34,65,17,21,13,
23,52,25,19,6,
32,28,16,14,10),3,5, byrow=T)
rownames(obscounts) <- c("line1", "line2", "line3")
colnames(obscounts) <- c("blemish", "crack", "improperTab", "missingTab", "other")
obscounts
chisq.test(obscounts)
# For Text problem 8.52, a random sample of individuals who drive to work in a
# large metropolitan area was obtained, and each individual was categorized with
# respect to both size of vehicle and commuting distance (in miles).
#
# The null hypothesis is that vehicle and commuting distance are independent.
# The alternative hypothesis is that vehicle and commuting distance or dependent.
#
# The data are given in obscounts below:
# The rows correspond to vehicle type (subcompact, compact, midsize, fullsize).
# The columns correspond to distance (0-<10,10-<20,>=20).
obscounts<-matrix(c(6,27,19,
8,36,17,
21,45,33,
14,18,6),4,3, byrow=T)
obscounts
chisq.test(obscounts)
# Here is the above chi-squared test "by hand," i.e., without chisq.test():
total = sum(obscounts)
rowsum = apply(obscounts,1,sum)
colsum = apply(obscounts,2,sum)
expected = (matrix(rowsum) %*% t(matrix(colsum))) / total
expected      # Don't worry about the details of the above %*% operation.
resid = (obscounts-expected)/sqrt(expected)
df = prod(dim(obscounts)-1)  # This df is just the product (nrow-1)(ncol-1)
X2 = sum(resid^2)            # = observed X-squared.
1-pchisq(X2,df)    # p-value = area under the chi-squared distribution
dat = read.table("http://faculty.washington.edu/lynb/StatMath390/9_1_dat.txt",header=TRUE)
aov.1 = aov(Vibration~ as.factor(Brand), data=dat)
# Remember "as.factor" is important. It tells R to treat Brand as a categorical variable
# with categories 1, 2, ..., 5
# instead of as a continuous variable with values 1, ..., 5
# With as.factor(Brand), the F-test has 4 numerator degrees of freedom (for 5-1 categories)
summary(aov.1)  # you can compare this to Table 9.1 in the textbook, or to doing it by hand further below
# NOTE: If you omit, "as.factor", then the F-test has only 1 numerator degrees of freedom
# It is NOT a test of the null hypothesis that the mean vibration is the same across brands!
summary(aov(Vibration~ (Brand), data=dat))
# Look at the summary of the test of the null hypothesis that the mean vibration is the same across brands
summary(aov.1)
boxplot(Vibration ~ Brand, data=dat)
dat = read.table("http://faculty.washington.edu/lynb/StatMath390/9_1_dat.txt",header=TRUE)
attach(dat)
# If you have only means and standard deviations from data, then R does not know
# how to do ANOVA. Then, you must do it "by hand," i.e., using the basic formulas.
#
# For doing a 1-way ANOVA test, let's compute the mean and standard deviation
# for the data in Table 9.1, then use the basic ANOVA equations to show that we
# get the same answers as above.
?attach
k = 5                          # number of categories.
n = m = s = numeric(k)         # space for mean and sd in each category.
for(i in 1:k){
n[i] = length(dat[Brand==i,2])                        # sample size in each category, and
m[i] = mean(dat[Brand==i,2])   # mean in each category, and
s[i] = sd(dat[Brand==i,2])     # standard dev in each category.
}                              # Ignore R warnings, if any.
n
m
s
# Let's do ANOVA by hand (i.e. using the basic anova equations):
N = sum(n)                       # total number of observations
k = length(n)                    # number of categories.
df1 = k-1                        # numerator df.
df2 = N-k                        # denominator df.
grand.ave = sum(m * n) / N       # grand average.
SSB = sum(n*(m - grand.ave)^2)   # sum-squared between groups.
SSW = sum((n-1)*s^2 )            # sum-squared within groups. Recall that s^2 = sum of squares / (n-1).
MSB = SSB/df1                    # mean-squared between groups.
MSW = SSW/df2                    # mean-squared within groups.
F = MSB/MSW                      # F-ratio
p.value = 1-pf(F,df1,df2)        # p-value. Note pf(F,df1,df2) and pf(F,df2,df1) give different results!
df1 ; df2 ; SSB ; SSW ; MSB ; MSW ; F ; p.value   # same as anova table above.
# Look at the summary of the test of the null hypothesis that the mean vibration is the same across brands
summary(aov.1)
# inflated and the remainder (1181-882 = 299) were contricted.
# Is there evidence to contradict Mendel's theory?
# This is a test of 2 proportions (proportion inflated, proportion constricted)
# from one population.
# The null hypothesis is that the proportion inflated is 0.75 and
# the proportion constricted is 0.25.
# The alternative is that the null is not true.
# In lecture we tested this with a "Z-test" and also a chi-square test.
# (With only 2 categories, the z-test and chi-square test are the same.)
# Here, we use the chi-square test.
obscounts = c(60,10,30)      # Note the data are entered as *counts*.
pi0 = c(.33333, .33333, .33333) # But the null values are given as proportions.
chisq.test(obscounts,p=pi0 ) # This can be confusing. Be careful!
# inflated and the remainder (1181-882 = 299) were contricted.
# Is there evidence to contradict Mendel's theory?
# This is a test of 2 proportions (proportion inflated, proportion constricted)
# from one population.
# The null hypothesis is that the proportion inflated is 0.75 and
# the proportion constricted is 0.25.
# The alternative is that the null is not true.
# In lecture we tested this with a "Z-test" and also a chi-square test.
# (With only 2 categories, the z-test and chi-square test are the same.)
# Here, we use the chi-square test.
obscounts = c(60,10,30)      # Note the data are entered as *counts*.
pi0 = c(1/3, 1/3, 1/3) # But the null values are given as proportions.
chisq.test(obscounts,p=pi0 ) # This can be confusing. Be careful!
col1 <- c(1,1,2,2,3,3,4,4)
col2 <- c(30.8, 27.2, 24.7, 26.1, 25.1, 23.8, 24.5, 29.8)
col1 <- c(1,1,2,2,3,3,4,4)
col2 <- c(30.8, 27.2, 24.7, 26.1, 25.1, 23.8, 24.5, 29.8)
df <- data.frame(col1, col2)
colnames(df) <- c("manufacturer", "foam_density")
View(df)
aov.c6.3 <- aov(df$col2 ~ as.factor(df$col1), data=df)
aov.c6.3 <- aov(foam_density ~ as.factor(manufacturer), data=df)
col1 <- c(1,1,2,2,3,3,4,4)
col2 <- c(30.8, 27.2, 24.7, 26.1, 25.1, 23.8, 24.5, 29.8)
df <- data.frame(col1, col2)
colnames(df) <- c("manufacturer", "foam_density")
aov.2 <- aov(foam_density ~ as.factor(manufacturer), data=df)
summary(aov.2)
q11.4x <- c(2, 12, 14, 17, 23, 30, 40, 47,
55, 67, 72, 81, 96, 112, 27)
q11.4y <- c(4, 10, 13, 15, 15, 25, 27, 46,
38, 46, 53, 70, 82, 99, 100)
plot(q11.4x, q11.4y)
summary(q11.4y ~ q11.4x)
lm(q11.4y ~ q11.4x)
summmary(lm(q11.4y ~ q11.4x))
summary(lm(q11.4y ~ q11.4x))
q11.4x <- c(5, 12, 14, 17, 23, 30, 40, 47,
55, 67, 72, 81, 96, 112, 27)
q11.4y <- c(4, 10, 13, 15, 15, 25, 27, 46,
38, 46, 53, 70, 82, 99, 100)
plot(q11.4x, q11.4y)
lm(q11.4y ~ q11.4x)
q11.4x <- c(5, 12, 14, 17, 23, 30, 40, 47,
55, 67, 72, 81, 96, 112, 127)
q11.4y <- c(4, 10, 13, 15, 15, 25, 27, 46,
38, 46, 53, 70, 82, 99, 100)
plot(q11.4x, q11.4y)
lm(q11.4y ~ q11.4x)
summary(lm(q11.4y ~ q11.4x))
anova(lm(q11.4y ~ q11.4x))
q11.4x <- c(5, 12, 14, 17, 23, 30, 40, 47,
55, 67, 72, 81, 96, 112, 127)
q11.4y <- c(4, 10, 13, 15, 15, 25, 27, 46,
38, 46, 53, 70, 82, 99, 100)
plot(q11.4x, q11.4y)
lm(q11.4y ~ q11.4x)
summary(lm(q11.4y ~ q11.4x))
anova(lm(q11.4y ~ q11.4x))
fit = lm(q11.4y ~ q11.4x)
summary(fit)$r.square
sse = sum((fitted(fit) - mean(q11.4x))^2)
sse = sum((fitted(fit) - mean(q11.4y))^2)
sxx <- sum((q11.4x-mean(q11.4x))^2)
sxx
q11.22x <- c(.11,.13,.14,.18,.29,.44,.67,.78,.93)
q11.22y <- c(1.72,2.17,2.33,3,5.17,7.61,11.17,12.72,14.78)
summary(lm(q11.22y ~ q11.22x))
mean(q11.22x)
16.0593 * 0.4077778 + 0.1925
(16.0593 * 0.4077778 + 0.1925) / 2
plot(q11.22x, q11.22y)
mean(q11.22x)
sxx <- sum((q11.22x-mean(q11.22x))^2)
sxx
sxx <- sum((q11.4x-mean(q11.4x))^2)
sxx
q11.4x <- c(5, 12, 14, 17, 23, 30, 40, 47,
55, 67, 72, 81, 96, 112, 127)
q11.4y <- c(4, 10, 13, 15, 15, 25, 27, 46,
38, 46, 53, 70, 82, 99, 100)
sxx <- sum((q11.4x-mean(q11.4x))^2)
sxx
summary(lm(q11.4y ~ q11.4x))
# 11.41
depth <- c(8.9, 36.6, 36.8, 6.1, 6.9, 6.9, 7.3, 8.4, 6.5, 8, 4.5, 9.9, 2.9, 2)
watcont <- c(31.5, 27, 25.9, 39.1, 39.2, 38.3, 33.9, 33.8, 27.9, 33.1, 26.3, 37.8, 34.6, 36.4)
shstren <- c(14.7, 48, 25.6, 10, 16, 16.8, 20.7, 38.8, 16.9, 27, 16, 24.9, 7.3, 12.8)
fit.simple <- lm(shstren~watcont+depth)
fit.complex <- lm(shstren~watcont+depth+watcont^2+depth^2+(watcont*depth))
anova(fit.complex,fit.simple)
anova(fit.simple,fit.complex)
summary(fit.simple)
fit.simple <- lm(shstren~watcont+depth)
fit.complex <- lm(shstren~watcont+depth+watcont^2+depth^2+(watcont*depth))
anova(fit.complex,fit.simple)
anova(fit.simple, fit.complex)
fit.simple <- lm(shstren~watcont+depth)
fit.complex <- lm(shstren~(watcont+depth+watcont^2+depth^2+(watcont*depth)))
anova(fit.simple, fit.complex)
# C7.1
ec_level <- c(1.6, 3.8, 6, 10.2)
tomato1.6 <- c(59.5, 53.3, 56.8, 63.1, 58.7)
tomato3.8 <- c(55.2, 59.1, 52.8, 54.5)
tomato6 <- c(51.7, 48.8, 53.9, 49)
tomato10.2 <- c(60.6, 58.5, 55, 65.2, 61.3)
tomato_means <- c(mean(tomato1.6), mean(tomato3.8), mean(tomato6), mean(tomato10.2))
anova(ec_level~tomato_means)
anova(lm(ec_level~tomato_means))
# C7.1
ec_level <- c(1.6, 3.8, 6, 10.2)
tomato1.6 <- c(59.5, 53.3, 56.8, 63.1, 58.7)
tomato3.8 <- c(55.2, 59.1, 52.8, 54.5)
tomato6 <- c(51.7, 48.8, 53.9, 49)
tomato10.2 <- c(60.6, 58.5, 55, 65.2, 61.3)
tomato_means <- c(mean(tomato1.6), mean(tomato3.8), mean(tomato6), mean(tomato10.2))
anova(lm(ec_level~tomato_means))
?anova
ec_level <- c(1.6, 3.8, 6, 10.2)
tomato1.6 <- c(59.5, 53.3, 56.8, 63.1, 58.7)
tomato3.8 <- c(55.2, 59.1, 52.8, 54.5)
tomato6 <- c(51.7, 48.8, 53.9, 49)
tomato10.2 <- c(60.6, 58.5, 55, 65.2, 61.3)
tomato_means <- c(mean(tomato1.6), mean(tomato3.8), mean(tomato6), mean(tomato10.2))
anova(lm(ec_level~tomato_means))
ec_levels <- c(1.6, 1.6, 1.6, 1.6, 1.6, 3.8, 3.8, 3.8, 3.8, 6, 6, 6, 6, 10.2, 10.2, 10.2, 10.2, 10.2)
tomatoes <- (c(tomato1.6, tomato3.8, tomato6, tomato10.2))
summary(lm(ec_levels ~ tomatoes))
summary(lm(tomatoes ~ ec_levels))
summary(lm(ec_levels ~ tomatoes))
summary(lm(tomatoes ~ ec_levels))
summary(lm(ec_levels ~ tomatoes))
summary(lm(tomatoes ~ ec_levels))
summary(lm(ec_levels ~ tomatoes))
summary(lm(tomatoes ~ ec_levels))
library(googleway)
source("server.R")
setwd("~/Desktop/Autumn 2018/info/FINAL PROJECT/Crime-vs-Weather")
library(googleway)
source("server.R")
View(crime)
select(crime$Occurred.Date, contains"1973")
select(crime$Occurred.Date, contains("1973"))
library(dplyr)
select(crime$Occurred.Date, contains("1973"))
filter(crime, grepl('1975', Occured.Date))
filter(crime, grepl('1975', Occurred.Date))
View(filter(crime, grepl('1975', Occurred.Date)))
new <- filter(crime, grepl('1975', Occurred.Date))
View(new)
new <- filter(crime, grepl('1998', Occurred.Date))
View(new)
new <- filter(crime, grepl('1998:1999', Occurred.Date))
View(new)
new <- filter(crime, grepl(1998:1999, Occurred.Date))
range <- number_range(1973, 2001)
install.packages("rebus")
library(rebus)
range <- number_range(1973, 2001)
new <- filter(crime, grepl(range), Occurred.Date)
?filter()
?filter
new <- filter(crime, grepl("2001|2002|2003|2004|2005|2006|2007|2008|2009|2010|
2011|2012|2013|2014|2015|2016|2017"), Occurred.Date)
View(new)
new <- filter(crime, grepl("2001|2002|2003|2004|2005|2006|2007|2008|2009|2010|
2011|2012|2013|2014|2015|2016|2017", Occurred.Date))
View(new)
random_sample <- new[sample(nrow(new), 50), ]
View(random_sample)
View(random_sample)
google_map(data = random_sample) %>%
add_markers(lat = "Latitude", lon = "Longitude")
set_key("AIzaSyDqLF7g98p9FAMfgfkpB-SpgOSZGPBglsY")
google_map(data = random_sample) %>%
add_markers(lat = "Latitude", lon = "Longitude")
random_sample <- crime[sample(nrow(crime), 50), ]
google_map(data = random_sample) %>%
add_markers(lat = "Latitude", lon = "Longitude")
google_map(data = washington) %>%
add_markers(lat = "Latitude", lon = "Longitude")
library(googleway)
library(dplyr)
source("server.R")
library(googleway)
library(dplyr)
source("server.R")
source("server.R")
set_key("AIzaSyDqLF7g98p9FAMfgfkpB-SpgOSZGPBglsY")
new <- filter(crime, grepl("2001|2002|2003|2004|2005|2006|2007|2008|2009|2010|
2011|2012|2013|2014|2015|2016|2017", Occurred.Date))
random_sample <- crime[sample(nrow(crime), 50), ]
google_map(data = random_sample) %>%
add_markers(lat = "Latitude", lon = "Longitude")
google_map(data = washington) %>%
add_markers(lat = "Latitude", lon = "Longitude")
View(new)
google_map(data = washington) %>%
add_markers(lat = "lat", lon = "long")
?add_markers
map_key <- "AIzaSyDqLF7g98p9FAMfgfkpB-SpgOSZGPBglsY"
google_map(data = washington, key = map_key) %>%
add_markers(lat = "lat", lon = "long")
View(washinton)
View(washington)
set_key("AIzaSyDqLF7g98p9FAMfgfkpB-SpgOSZGPBglsY")
google_map(data = washington) %>%
add_markers(lat = "lat", lon = "long")
google_map(data = washington) %>%
add_circles(lat = "lat", lon = "long")
google_map(data = washington) %>%
add_markers(lat = "lat", lon = "long")
google_map(data = washington) %>%
add_circles(lat = "lat", lon = "long")
set_key("AIzaSyDqLF7g98p9FAMfgfkpB-SpgOSZGPBglsY")
google_map(data = washington) %>%
add_markers(lat = "lat", lon = "long")
google_map(data = random_sample) %>%
add_circles(lat = "lat", lon = "long")
google_map(data = random_sample) %>%
add_markers(lat = "lat", lon = "long")
