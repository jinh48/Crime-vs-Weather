# probability for type II error: 1 - power
(beta <- 1-pow)
sigma <- 15    # theoretical standard deviation
mu0   <- 100   # expected value under H0
mu1   <- 101   # expected value under H1
alpha <- 0.01  # probability of type I error
# critical value for a level alpha test
crit <- qnorm(1-alpha, mu0, sigma)
# power: probability for values > critical value under H1
(pow <- pnorm(crit, mu1, sigma, lower.tail=TRUE))
# probability for type II error: 1 - power
(beta <- 1-pow)
sigma <- 15    # theoretical standard deviation
mu0   <- 100   # expected value under H0
mu1   <- 100   # expected value under H1
alpha <- 0.01  # probability of type I error
# critical value for a level alpha test
crit <- qnorm(1-alpha, mu0, sigma)
# power: probability for values > critical value under H1
(pow <- pnorm(crit, mu1, sigma, lower.tail=FALSE))
# probability for type II error: 1 - power
(beta <- 1-pow)
sigma <- 15    # theoretical standard deviation
mu0   <- 100   # expected value under H0
mu1   <- 101   # expected value under H1
alpha <- 0.01  # probability of type I error
# critical value for a level alpha test
crit <- qnorm(1-alpha, mu0, sigma)
# power: probability for values > critical value under H1
(pow <- pnorm(crit, mu1, sigma, lower.tail=FALSE))
# probability for type II error: 1 - power
(beta <- 1-pow)
# EXAMPLE 2
n = 100                # sample size
sigma = 15           # population standard deviation
sem = sigma/sqrt(n); sem   # standard error
alpha = .01           # significance level
mu0 = 101          # hypothetical lower bound
q = qnorm(alpha, mean=mu0, sd=sem); q
mu = 100             # assumed actual mean
pnorm(q, mean=mu, sd=sem, lower.tail=FALSE)
n = 100                    # sample size
sigma = 15                 # population standard deviation
sem = sigma/sqrt(n); sem   # standard error
alpha = .01                # significance level
mu0 = 101                  # hypothetical lower bound
q = qnorm(alpha, mean=mu0, sd=sem); q
mu = 100                   # assumed actual mean
pnorm(q, mean=mu, sd=sem, lower.tail=FALSE)
n = 400                    # sample size
sigma = 15                 # population standard deviation
sem = sigma/sqrt(n); sem   # standard error
alpha = .01                # significance level
mu0 = 101                  # hypothetical lower bound
q = qnorm(alpha, mean=mu0, sd=sem); q
mu = 100                   # assumed actual mean
pnorm(q, mean=mu, sd=sem, lower.tail=FALSE)
n = 1600                    # sample size
sigma = 15                 # population standard deviation
sem = sigma/sqrt(n); sem   # standard error
alpha = .01                # significance level
mu0 = 101                  # hypothetical lower bound
q = qnorm(alpha, mean=mu0, sd=sem); q
mu = 100                   # assumed actual mean
pnorm(q, mean=mu, sd=sem, lower.tail=FALSE)
n = 2500                    # sample size
sigma = 15                 # population standard deviation
sem = sigma/sqrt(n); sem   # standard error
alpha = .01                # significance level
mu0 = 101                  # hypothetical lower bound
q = qnorm(alpha, mean=mu0, sd=sem); q
mu = 100                   # assumed actual mean
pnorm(q, mean=mu, sd=sem, lower.tail=FALSE)
# inflated and the remainder (1181-882 = 299) were contricted.
# Is there evidence to contradict Mendel's theory?
# This is a test of 2 proportions (proportion inflated, proportion constricted)
# from one population.
# The null hypothesis is that the proportion inflated is 0.75 and
# the proportion constricted is 0.25.
# The alternative is that the null is not true.
# In lecture we tested this with a "Z-test" and also a chi-square test.
# (With only 2 categories, the z-test and chi-square test are the same.)
# Here, we use the chi-square test.
obscounts = c(882,299)      # Note the data are entered as *counts*.
pi0 = c(0.75, 0.25) # But the null values are given as proportions.
chisq.test(obscounts,p=pi0 ) # This can be confusing. Be careful!
# Text Example 8.9 presents the counts of reasons for which cans do not meet
# specification (blemish on can, crack in can, improper pull tab, pull tab missing,
# other) from 3 different can production lines (lines 1, 2, 3)
# The observed counts are provided as obscounts below.
# The rows correspond to production lines 1, 2 and 3.
# The columns correspond to the 5 reasons: blemish on can, crack in can,
# improper pull tab, pull tab missing, other
obscounts<-matrix(c(34,65,17,21,13,
23,52,25,19,6,
32,28,16,14,10),3,5, byrow=T)
rownames(obscounts) <- c("line1", "line2", "line3")
colnames(obscounts) <- c("blemish", "crack", "improperTab", "missingTab", "other")
obscounts
chisq.test(obscounts)
# For Text problem 8.52, a random sample of individuals who drive to work in a
# large metropolitan area was obtained, and each individual was categorized with
# respect to both size of vehicle and commuting distance (in miles).
#
# The null hypothesis is that vehicle and commuting distance are independent.
# The alternative hypothesis is that vehicle and commuting distance or dependent.
#
# The data are given in obscounts below:
# The rows correspond to vehicle type (subcompact, compact, midsize, fullsize).
# The columns correspond to distance (0-<10,10-<20,>=20).
obscounts<-matrix(c(6,27,19,
8,36,17,
21,45,33,
14,18,6),4,3, byrow=T)
obscounts
chisq.test(obscounts)
# Here is the above chi-squared test "by hand," i.e., without chisq.test():
total = sum(obscounts)
rowsum = apply(obscounts,1,sum)
colsum = apply(obscounts,2,sum)
expected = (matrix(rowsum) %*% t(matrix(colsum))) / total
expected      # Don't worry about the details of the above %*% operation.
resid = (obscounts-expected)/sqrt(expected)
df = prod(dim(obscounts)-1)  # This df is just the product (nrow-1)(ncol-1)
X2 = sum(resid^2)            # = observed X-squared.
1-pchisq(X2,df)    # p-value = area under the chi-squared distribution
dat = read.table("http://faculty.washington.edu/lynb/StatMath390/9_1_dat.txt",header=TRUE)
aov.1 = aov(Vibration~ as.factor(Brand), data=dat)
# Remember "as.factor" is important. It tells R to treat Brand as a categorical variable
# with categories 1, 2, ..., 5
# instead of as a continuous variable with values 1, ..., 5
# With as.factor(Brand), the F-test has 4 numerator degrees of freedom (for 5-1 categories)
summary(aov.1)  # you can compare this to Table 9.1 in the textbook, or to doing it by hand further below
# NOTE: If you omit, "as.factor", then the F-test has only 1 numerator degrees of freedom
# It is NOT a test of the null hypothesis that the mean vibration is the same across brands!
summary(aov(Vibration~ (Brand), data=dat))
# Look at the summary of the test of the null hypothesis that the mean vibration is the same across brands
summary(aov.1)
boxplot(Vibration ~ Brand, data=dat)
dat = read.table("http://faculty.washington.edu/lynb/StatMath390/9_1_dat.txt",header=TRUE)
attach(dat)
# If you have only means and standard deviations from data, then R does not know
# how to do ANOVA. Then, you must do it "by hand," i.e., using the basic formulas.
#
# For doing a 1-way ANOVA test, let's compute the mean and standard deviation
# for the data in Table 9.1, then use the basic ANOVA equations to show that we
# get the same answers as above.
?attach
k = 5                          # number of categories.
n = m = s = numeric(k)         # space for mean and sd in each category.
for(i in 1:k){
n[i] = length(dat[Brand==i,2])                        # sample size in each category, and
m[i] = mean(dat[Brand==i,2])   # mean in each category, and
s[i] = sd(dat[Brand==i,2])     # standard dev in each category.
}                              # Ignore R warnings, if any.
n
m
s
# Let's do ANOVA by hand (i.e. using the basic anova equations):
N = sum(n)                       # total number of observations
k = length(n)                    # number of categories.
df1 = k-1                        # numerator df.
df2 = N-k                        # denominator df.
grand.ave = sum(m * n) / N       # grand average.
SSB = sum(n*(m - grand.ave)^2)   # sum-squared between groups.
SSW = sum((n-1)*s^2 )            # sum-squared within groups. Recall that s^2 = sum of squares / (n-1).
MSB = SSB/df1                    # mean-squared between groups.
MSW = SSW/df2                    # mean-squared within groups.
F = MSB/MSW                      # F-ratio
p.value = 1-pf(F,df1,df2)        # p-value. Note pf(F,df1,df2) and pf(F,df2,df1) give different results!
df1 ; df2 ; SSB ; SSW ; MSB ; MSW ; F ; p.value   # same as anova table above.
# Look at the summary of the test of the null hypothesis that the mean vibration is the same across brands
summary(aov.1)
# inflated and the remainder (1181-882 = 299) were contricted.
# Is there evidence to contradict Mendel's theory?
# This is a test of 2 proportions (proportion inflated, proportion constricted)
# from one population.
# The null hypothesis is that the proportion inflated is 0.75 and
# the proportion constricted is 0.25.
# The alternative is that the null is not true.
# In lecture we tested this with a "Z-test" and also a chi-square test.
# (With only 2 categories, the z-test and chi-square test are the same.)
# Here, we use the chi-square test.
obscounts = c(60,10,30)      # Note the data are entered as *counts*.
pi0 = c(.33333, .33333, .33333) # But the null values are given as proportions.
chisq.test(obscounts,p=pi0 ) # This can be confusing. Be careful!
# inflated and the remainder (1181-882 = 299) were contricted.
# Is there evidence to contradict Mendel's theory?
# This is a test of 2 proportions (proportion inflated, proportion constricted)
# from one population.
# The null hypothesis is that the proportion inflated is 0.75 and
# the proportion constricted is 0.25.
# The alternative is that the null is not true.
# In lecture we tested this with a "Z-test" and also a chi-square test.
# (With only 2 categories, the z-test and chi-square test are the same.)
# Here, we use the chi-square test.
obscounts = c(60,10,30)      # Note the data are entered as *counts*.
pi0 = c(1/3, 1/3, 1/3) # But the null values are given as proportions.
chisq.test(obscounts,p=pi0 ) # This can be confusing. Be careful!
col1 <- c(1,1,2,2,3,3,4,4)
col2 <- c(30.8, 27.2, 24.7, 26.1, 25.1, 23.8, 24.5, 29.8)
col1 <- c(1,1,2,2,3,3,4,4)
col2 <- c(30.8, 27.2, 24.7, 26.1, 25.1, 23.8, 24.5, 29.8)
df <- data.frame(col1, col2)
colnames(df) <- c("manufacturer", "foam_density")
View(df)
aov.c6.3 <- aov(df$col2 ~ as.factor(df$col1), data=df)
aov.c6.3 <- aov(foam_density ~ as.factor(manufacturer), data=df)
col1 <- c(1,1,2,2,3,3,4,4)
col2 <- c(30.8, 27.2, 24.7, 26.1, 25.1, 23.8, 24.5, 29.8)
df <- data.frame(col1, col2)
colnames(df) <- c("manufacturer", "foam_density")
aov.2 <- aov(foam_density ~ as.factor(manufacturer), data=df)
summary(aov.2)
q11.4x <- c(2, 12, 14, 17, 23, 30, 40, 47,
55, 67, 72, 81, 96, 112, 27)
q11.4y <- c(4, 10, 13, 15, 15, 25, 27, 46,
38, 46, 53, 70, 82, 99, 100)
plot(q11.4x, q11.4y)
summary(q11.4y ~ q11.4x)
lm(q11.4y ~ q11.4x)
summmary(lm(q11.4y ~ q11.4x))
summary(lm(q11.4y ~ q11.4x))
q11.4x <- c(5, 12, 14, 17, 23, 30, 40, 47,
55, 67, 72, 81, 96, 112, 27)
q11.4y <- c(4, 10, 13, 15, 15, 25, 27, 46,
38, 46, 53, 70, 82, 99, 100)
plot(q11.4x, q11.4y)
lm(q11.4y ~ q11.4x)
q11.4x <- c(5, 12, 14, 17, 23, 30, 40, 47,
55, 67, 72, 81, 96, 112, 127)
q11.4y <- c(4, 10, 13, 15, 15, 25, 27, 46,
38, 46, 53, 70, 82, 99, 100)
plot(q11.4x, q11.4y)
lm(q11.4y ~ q11.4x)
summary(lm(q11.4y ~ q11.4x))
anova(lm(q11.4y ~ q11.4x))
q11.4x <- c(5, 12, 14, 17, 23, 30, 40, 47,
55, 67, 72, 81, 96, 112, 127)
q11.4y <- c(4, 10, 13, 15, 15, 25, 27, 46,
38, 46, 53, 70, 82, 99, 100)
plot(q11.4x, q11.4y)
lm(q11.4y ~ q11.4x)
summary(lm(q11.4y ~ q11.4x))
anova(lm(q11.4y ~ q11.4x))
fit = lm(q11.4y ~ q11.4x)
summary(fit)$r.square
sse = sum((fitted(fit) - mean(q11.4x))^2)
sse = sum((fitted(fit) - mean(q11.4y))^2)
sxx <- sum((q11.4x-mean(q11.4x))^2)
sxx
q11.22x <- c(.11,.13,.14,.18,.29,.44,.67,.78,.93)
q11.22y <- c(1.72,2.17,2.33,3,5.17,7.61,11.17,12.72,14.78)
summary(lm(q11.22y ~ q11.22x))
mean(q11.22x)
16.0593 * 0.4077778 + 0.1925
(16.0593 * 0.4077778 + 0.1925) / 2
plot(q11.22x, q11.22y)
mean(q11.22x)
sxx <- sum((q11.22x-mean(q11.22x))^2)
sxx
sxx <- sum((q11.4x-mean(q11.4x))^2)
sxx
q11.4x <- c(5, 12, 14, 17, 23, 30, 40, 47,
55, 67, 72, 81, 96, 112, 127)
q11.4y <- c(4, 10, 13, 15, 15, 25, 27, 46,
38, 46, 53, 70, 82, 99, 100)
sxx <- sum((q11.4x-mean(q11.4x))^2)
sxx
summary(lm(q11.4y ~ q11.4x))
# 11.41
depth <- c(8.9, 36.6, 36.8, 6.1, 6.9, 6.9, 7.3, 8.4, 6.5, 8, 4.5, 9.9, 2.9, 2)
watcont <- c(31.5, 27, 25.9, 39.1, 39.2, 38.3, 33.9, 33.8, 27.9, 33.1, 26.3, 37.8, 34.6, 36.4)
shstren <- c(14.7, 48, 25.6, 10, 16, 16.8, 20.7, 38.8, 16.9, 27, 16, 24.9, 7.3, 12.8)
fit.simple <- lm(shstren~watcont+depth)
fit.complex <- lm(shstren~watcont+depth+watcont^2+depth^2+(watcont*depth))
anova(fit.complex,fit.simple)
anova(fit.simple,fit.complex)
summary(fit.simple)
fit.simple <- lm(shstren~watcont+depth)
fit.complex <- lm(shstren~watcont+depth+watcont^2+depth^2+(watcont*depth))
anova(fit.complex,fit.simple)
anova(fit.simple, fit.complex)
fit.simple <- lm(shstren~watcont+depth)
fit.complex <- lm(shstren~(watcont+depth+watcont^2+depth^2+(watcont*depth)))
anova(fit.simple, fit.complex)
# C7.1
ec_level <- c(1.6, 3.8, 6, 10.2)
tomato1.6 <- c(59.5, 53.3, 56.8, 63.1, 58.7)
tomato3.8 <- c(55.2, 59.1, 52.8, 54.5)
tomato6 <- c(51.7, 48.8, 53.9, 49)
tomato10.2 <- c(60.6, 58.5, 55, 65.2, 61.3)
tomato_means <- c(mean(tomato1.6), mean(tomato3.8), mean(tomato6), mean(tomato10.2))
anova(ec_level~tomato_means)
anova(lm(ec_level~tomato_means))
# C7.1
ec_level <- c(1.6, 3.8, 6, 10.2)
tomato1.6 <- c(59.5, 53.3, 56.8, 63.1, 58.7)
tomato3.8 <- c(55.2, 59.1, 52.8, 54.5)
tomato6 <- c(51.7, 48.8, 53.9, 49)
tomato10.2 <- c(60.6, 58.5, 55, 65.2, 61.3)
tomato_means <- c(mean(tomato1.6), mean(tomato3.8), mean(tomato6), mean(tomato10.2))
anova(lm(ec_level~tomato_means))
?anova
ec_level <- c(1.6, 3.8, 6, 10.2)
tomato1.6 <- c(59.5, 53.3, 56.8, 63.1, 58.7)
tomato3.8 <- c(55.2, 59.1, 52.8, 54.5)
tomato6 <- c(51.7, 48.8, 53.9, 49)
tomato10.2 <- c(60.6, 58.5, 55, 65.2, 61.3)
tomato_means <- c(mean(tomato1.6), mean(tomato3.8), mean(tomato6), mean(tomato10.2))
anova(lm(ec_level~tomato_means))
ec_levels <- c(1.6, 1.6, 1.6, 1.6, 1.6, 3.8, 3.8, 3.8, 3.8, 6, 6, 6, 6, 10.2, 10.2, 10.2, 10.2, 10.2)
tomatoes <- (c(tomato1.6, tomato3.8, tomato6, tomato10.2))
summary(lm(ec_levels ~ tomatoes))
summary(lm(tomatoes ~ ec_levels))
summary(lm(ec_levels ~ tomatoes))
summary(lm(tomatoes ~ ec_levels))
summary(lm(ec_levels ~ tomatoes))
summary(lm(tomatoes ~ ec_levels))
summary(lm(ec_levels ~ tomatoes))
summary(lm(tomatoes ~ ec_levels))
library(shiny)
library(plotly)
library(dplyr)
library(lubridate)
library(ggplot2)
library(ggmap)
library(data.table)
library(ggrepel)
# library(rworldmap)
# library(rworldxtra)
library(maps)
library(mapdata)
source("process_data.R")
# View(crime)
# ---- SETTING UP WASHINGTON CRIME DATA ---- #
states <- map_data("state")
washington <- subset(states, region == "washington")
counties <- map_data("county")
wa_county <- subset(counties, region == "washington")
washington_base <- ggplot(data = washington, mapping = aes(x = long, y = lat, group = group)) +
geom_polygon(fill = "palegreen", color = "black") +
coord_fixed(xlim = c(-123, -121), ylim = c(47,48), ratio = 1.3) +
theme_nothing() +
geom_polygon(data = wa_county, fill = NA, color = "white") +
geom_polygon(color = "black", fill = NA) +
geom_point(data = crime, mapping = aes(x = crime$Longitude, y = crime$Latitude),
color = "red", inherit.aes = FALSE) +
geom_tile(aes(fill = weather)) +
scale_fill_gradient(low = "darkgreen", high = "lightgreen")
states <- map_data("state")
washington <- subset(states, region == "washington")
counties <- map_data("county")
wa_county <- subset(counties, region == "washington")
washington_base <- ggplot(data = washington, mapping = aes(x = long, y = lat, group = group)) +
geom_polygon(fill = "palegreen", color = "black") +
coord_fixed(xlim = c(-123, -121), ylim = c(47,48), ratio = 1.3) +
theme_nothing() +
geom_polygon(data = wa_county, fill = NA, color = "white") +
geom_polygon(color = "black", fill = NA) +
geom_point(data = crime, mapping = aes(x = crime$Longitude, y = crime$Latitude),
color = "red", inherit.aes = FALSE) +
geom_tile(aes(fill = weather)) #+
#scale_fill_gradient(low = "darkgreen", high = "lightgreen")
#scale_fill_gradient(low = "darkgreen", high = "lightgreen")
washington_base
weather
library("dplyr")
library("tidyr")
weather <- read.csv("data/weather.csv", header = TRUE, stringsAsFactors = FALSE)
crime <- read.csv("data/crime.csv", header = TRUE, stringsAsFactors = FALSE)
weather
setwd("~/Desktop/Autumn 2018/info/FINAL PROJECT/Crime-vs-Weather")
library("dplyr")
library("tidyr")
weather <- read.csv("data/weather.csv", header = TRUE, stringsAsFactors = FALSE)
crime <- read.csv("data/crime.csv", header = TRUE, stringsAsFactors = FALSE)
weather
weather
View(weather)
runApp()
runApp()
runApp()
runApp()
?pie
spring_slices <- make_slices(spring)
library(shiny)
library(plotly)
library(dplyr)
library(lubridate)
library(ggplot2)
library(ggmap)
library(data.table)
library(ggrepel)
# library(rworldmap)
# library(rworldxtra)
library(maps)
library(mapdata)
source("process_data.R")
# View(crime)
# ---- SETTING UP WASHINGTON CRIME DATA ---- #
states <- map_data("state")
washington <- subset(states, region == "washington")
counties <- map_data("county")
wa_county <- subset(counties, region == "washington")
washington_base <- ggplot(data = washington, mapping = aes(x = long, y = lat, group = group)) +
geom_polygon(fill = "palegreen", color = "black") +
coord_fixed(xlim = c(-123, -121), ylim = c(47,48), ratio = 1.3) +
theme_nothing() +
geom_polygon(data = wa_county, fill = NA, color = "white") +
geom_polygon(color = "black", fill = NA) +
geom_point(data = crime, mapping = aes(x = crime$Longitude, y = crime$Latitude),
color = "red", inherit.aes = FALSE) #+
# geom_tile(aes(fill = weather)) #+
#scale_fill_gradient(low = "darkgreen", high = "lightgreen")
washington_base
# ---- SETTING UP PIE CHARTS FOR SEASON ---- #
# Setting up the pie charts:
# create a new data frame for the purpose of the pie charts
crime_pie <- crime
# delete the year from all dates
crime_pie$Occurred.Date <- substring(crime_pie$Occurred.Date, 1, 5)
# change dates to Date format (automatically changes all years to 2018,
# so we can select by month & day only)
crime_pie$Occurred.Date <- as.Date(crime_pie$Occurred.Date, format = "%m/%d")
# separate data by season
spring <- crime_pie[crime_pie$Occurred.Date >= "2018-03-01" & crime_pie$Occurred.Date <= "2018-05-31",]
summer <- crime_pie[crime_pie$Occurred.Date >= "2018-06-01" & crime_pie$Occurred.Date <= "2018-09-01",]
autumn <- crime_pie[crime_pie$Occurred.Date >= "2018-10-01" & crime_pie$Occurred.Date <= "2018-11-30",]
winter <- crime_pie[crime_pie$Occurred.Date >= "2018-12-01" | crime_pie$Occurred.Date <= "2018-02-28",]
# create reusable function to make list of slice values for each season
make_slices <- function(df) {
df <- aggregate(df, by = list(df$Crime.Subcategory), FUN = NROW)
vector <- df[,3]
}
spring_slices <- make_slices(spring)
summer_slices <- make_slices(summer)
autumn_slices <- make_slices(spring)
winter_slices <- make_slices(winter)
spring_sum <- sum(spring_slices)
summer_sum <- sum(summer_slices)
autumn_sum <- sum(autumn_slices)
winter_sum <- sum(winter_slices)
runApp()
?assign
runApp()
runApp()
?do.call
slices <- do.call(paste0(string, "_slices"))
runApp()
runApp()
runApp()
# weather <- read.csv("data/weather.csv", header = TRUE, stringsAsFactors = FALSE)
crime <- read.csv("data/crime.csv", header = TRUE, stringsAsFactors = FALSE)
rain <- read.csv("data/rain.csv", header = TRUE, stringsAsFactors = FALSE)
View(rain)
rain_stacked <- stack(rain)
View(rain_stacked)
rain_stacked$date <- rep_len(rain_stacked[1,1]:rain_stacked[1,175])
dates <- rain_stacked$values[1,]:rain_stacked[175,]
dates <- rain_stacked[1:175,]
View(dates)
dates <- rain_stacked[1:175,1]
rain_stacked$date <- rep_len(dates, length.out = 3150)
# replace rain gauge # with its corresponding lat/long
rain_stacked$ind <- sub("RG01", "47.725033 -122.341384", rain_stacked$ind)
# weather <- read.csv("data/weather.csv", header = TRUE, stringsAsFactors = FALSE)
crime <- read.csv("data/crime.csv", header = TRUE, stringsAsFactors = FALSE)
rain <- read.csv("data/rain.csv", header = TRUE, stringsAsFactors = FALSE)
rain_stacked <- stack(rain)
# store all dates in a vector
dates <- rain_stacked[1:175,1]
# replicate dates for each set of measurements
rain_stacked$date <- rep_len(dates, length.out = 3150)
# replace rain gauge # with its corresponding lat/long
rain_stacked$ind <- sub("RG01", "47.725033 -122.341384", rain_stacked$ind)
rain_stacked$ind <- sub("RG02", "47.684385, -122.260105", rain_stacked$ind)
rain_stacked$ind <- sub("RG03", "47.657997, -122.317634", rain_stacked$ind)
rain_stacked$ind <- sub("RG04", "47.692621, -122.314398", rain_stacked$ind)
rain_stacked$ind <- sub("RG05", "47.508121, -122.387062", rain_stacked$ind) # no RG06
rain_stacked$ind <- sub("RG07", "47.699242, -122.370607", rain_stacked$ind)
rain_stacked$ind <- sub("RG08", "47.668468, -122.386092", rain_stacked$ind)
rain_stacked$ind <- sub("RG09", "47.674222, -122.355418", rain_stacked$ind)
rain_stacked$ind <- sub("RG10", "47.518854, -122.266914", rain_stacked$ind)
rain_stacked$ind <- sub("RG11", "47.618028, -122.358532", rain_stacked$ind)
rain_stacked$ind <- sub("RG12", "47.643205, -122.393554", rain_stacked$ind) # no RG13
rain_stacked$ind <- sub("RG14", "47.583939, -122.382968", rain_stacked$ind)
rain_stacked$ind <- sub("RG15", "47.565439, -122.339688", rain_stacked$ind)
rain_stacked$ind <- sub("RG16", "47.527203, -122.304678", rain_stacked$ind)
rain_stacked$ind <- sub("RG17", "47.516310, -122.318071", rain_stacked$ind)
rain_stacked$ind <- sub("RG18", "47.545982, -122.268642", rain_stacked$ind) # no RG19-24
rain_stacked$ind <- sub("RG25", "47.614766, -122.294049", rain_stacked$ind)
# weather <- read.csv("data/weather.csv", header = TRUE, stringsAsFactors = FALSE)
crime <- read.csv("data/crime.csv", header = TRUE, stringsAsFactors = FALSE)
rain <- read.csv("data/rain.csv", header = TRUE, stringsAsFactors = FALSE)
rain <- stack(rain)
# store all dates in a vector
dates <- rain[1:175,1]
# replicate dates for each set of measurements
rain$date <- rep_len(dates, length.out = 3150)
# delete first 175 - only contain dates, no data
rain = df[-1:-175,]
View(rain)
library("dplyr")
library("tidyr")
# weather <- read.csv("data/weather.csv", header = TRUE, stringsAsFactors = FALSE)
crime <- read.csv("data/crime.csv", header = TRUE, stringsAsFactors = FALSE)
library("dplyr")
library("tidyr")
# weather <- read.csv("data/weather.csv", header = TRUE, stringsAsFactors = FALSE)
crime <- read.csv("data/crime.csv", header = TRUE, stringsAsFactors = FALSE)
q()
